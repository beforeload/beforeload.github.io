<!DOCTYPE HTML>
<html>
<head>
  <meta charset="utf-8">
  <meta name="baidu-site-verification" content="OPRXXGXcV6onVshr" />
  
  <title>Hadoop | beforeload</title>
  <meta name="author" content="beforeload">
  
  <meta name="description" content="熟悉javascript，能写nodejs，看过Ruby，学过Java，对python也有兴趣，目前专注于C，搞搞Hadoop">
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  
  <meta property="og:site_name" content="beforeload"/>

  
    <meta property="og:image" content="undefined"/>
  

  <link href="/favicon.png" rel="icon">
  <link rel="alternate" href="http://beforeload.github.io/atom.xml" title="beforeload" type="application/atom+xml">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->
  <script src="//ajax.googleapis.com/ajax/libs/jquery/1.8/jquery.min.js"></script>
  
<script type="text/javascript">
  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-40059659-1']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();
</script>

</head>


<body>
  <header id="header" class="inner"><div class="alignleft">
  <h1><a href="/">beforeload</a></h1>
  <h2><a href="/">对编程世界充满激情的少年</a></h2>
</div>
<nav id="main-nav" class="alignright">
  <ul>
    
      <li><a href="/">Home</a></li>
    
      <li><a href="/archives">Archives</a></li>
    
      <li><a href="/atom.xml">RSS</a></li>
    
  </ul>
  <div class="clearfix"></div>
</nav>
<div class="clearfix"></div></header>
  <div id="content" class="inner">
    <div id="main-col" class="alignleft"><div id="wrapper">
<h2 class="archive-title tag">Hadoop</h2>


  
    <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2013-04-18T04:26:32.000Z"><a href="/2013/04/18/invertedindex-in-hadoop/">Apr 18 2013</a></time>
      
      
  
    <h1 class="title"><a href="/2013/04/18/invertedindex-in-hadoop/">Inverted Index in Hadoop</a></h1>
  

    </header>
    <div class="entry">
      
        <h2>倒排索引</h2>
<h3>简介:</h3>
<p>倒排索引是文档检索系统中最常用的数据结构，被广泛地应用于全文搜索引擎。它主要是用来存储某个单词(或词组)在一个文档或一组文档中的存储位置的映射，即提供了一种根据内容来查找文档的方式。由于不是更具文档来确定文档包含的内容，而是进行想法的操作，因而成为倒排索引(Inverted Index)。[1]</p>
<p>我的理解就是找到单词出现的文档的名称，多数情况下为一个列表。</p>
<p>一个单词可能在不同的文件中出现，所以我们需要定义一个权重，表示单词(即搜索的内容)跟文档的<strong><em>相关度</em></strong>。相关度的衡量多数情况下用<strong><em>词频</em></strong>来表示。</p>
<p>更加复杂的算法TF-IDF(Term Frequency-Inverse Document Frequency)统计单词在多少个文档中出现，甚至考虑单词在文档中出现的位置(例如标题处反应这个单词的重要性)。</p>
<p>理论的东西到此结束，下面写一下倒排索引的设计与实现。</p>
<h3>问题分析：</h3>
<p>信息的关键： <strong>单词</strong>，<strong>文档URI</strong>及<strong>词频</strong></p>
<h3>设计：</h3>
<h4>Map过程：</h4>
<p>TextInputFormat: 输入文件处理 -&gt; 文本每行的偏移量及其内容<br><code>&lt;key, value&gt;</code> =&gt;  单词，文档URI和词频<br>两个值对应三个值，需要增加Combine过程进行词频统计。</p>
<p><strong><em>key</em></strong>： <strong>单词:URI</strong> (例如：MapReduce:1.txt)<br><strong><em>value</em></strong>：<strong>词频</strong>，相同单词词频组成列表传递给Combiner过程，实现的功能类似于WordCount      </p>
<h4>Combine过程：</h4>
<p>Combine过程会把相同的key值对应的value值累加<br>Map过程得到的结果为    </p>
<figure class="highlight lang-Java"><table><tr><td class="gutter"><pre>1
2
3
4
</pre></td><td class="code"><pre><span class="string">"MapRuduce:file01.txt"</span>  list(<span class="number">1</span>)          =&gt;     <span class="string">"MapReduce:file01.txt"</span>  <span class="number">1</span>    
<span class="string">"is:file01.txt"</span>         list(<span class="number">1</span>,<span class="number">1</span>)        =&gt;     <span class="string">"is:file01.txt"</span>         <span class="number">2</span>    
<span class="string">"powerful:file01.txt"</span>   list(<span class="number">1</span>)          =&gt;     <span class="string">"powerful:file01.txt"</span>   <span class="number">1</span>    
<span class="string">"simple:file01.txt"</span>     list(<span class="number">1</span>)          =&gt;     <span class="string">"simple:file01.txt"</span>     <span class="number">1</span>   
</pre></td></tr></table></figure>

<p><strong><em>key</em></strong>： <strong>单词</strong>
<strong><em>value</em></strong>: <strong>URI:词频</strong>(如：1.txt:1)</p>
<p><strong>好处</strong>：可以利用MapReduce框架默认的HashPartitioner类完成Shuffle过程。</p>
<h4>Reduce过程：</h4>
<p>Combiner过程就已经把相同的单词的所有记录发送给同一个Reducer进行处理，Reduce过程就变得很简单，只需要将相同的key和value值组合成倒排索引文件所需的格式即可，剩下的交给MapReducer框架自动完成。</p>
<h4>问题</h4>
<ol>
<li>文件数目;</li>
<li>文件大小;</li>
<li>Reduce过程没有统计词频，有可能会造成词频未统计完全的单词。    </li>
</ol>
<p><strong>备注及解决办法：</strong></p>
<ol>
<li>单个文件不宜过大，具体值与默认HDFS块大小及相关配置有关；</li>
<li>重写InputFormat类将每个文件作为一个split；</li>
<li>执行两次MapReduce，第一次统计词频，第二次MapReduce用于生成倒排索引。</li>
</ol>
<h4>优化思路：</h4>
<ol>
<li>利用复合键值对等实现包含更多信息的倒排索引。</li>
</ol>
<p><strong><strong>附Java源码：</strong></strong></p>
<figure class="highlight lang-Java"><table><tr><td class="gutter"><pre>1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
</pre></td><td class="code"><pre><span class="keyword">import</span> java.io.IOException;
<span class="keyword">import</span> java.util.StringTokenizer;

<span class="keyword">import</span> org.apache.hadoop.conf.Configuration;
<span class="keyword">import</span> org.apache.hadoop.fs.Path;
<span class="keyword">import</span> org.apache.hadoop.io.Text;
<span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;
<span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;
<span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;
<span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
<span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileSplit;
<span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
<span class="keyword">import</span> org.apache.hadoop.util.GenericOptionsParser;

<span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">InvertedIndex</span> {</span>
	<span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">InvertedIndexMapper</span> <span class="keyword">extends</span>
			<span class="title">Mapper</span>&<span class="title">lt</span>;<span class="title">Object</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">Text</span>&<span class="title">gt</span>; {</span>
		<span class="keyword">private</span> Text keyInfo = <span class="keyword">new</span> Text(); <span class="comment">// 存放单词和URI的组合</span>
		<span class="keyword">private</span> Text valueInfo = <span class="keyword">new</span> Text(); <span class="comment">// 存储词频</span>
		<span class="keyword">private</span> FileSplit split; <span class="comment">// 存储Split对象</span>

		<span class="keyword">public</span> <span class="keyword">void</span> map(Object key, Text value, Context context)
				<span class="keyword">throws</span> IOException, InterruptedException {

			<span class="comment">// 获得&lt;key, value&gt;对所属的FileSplit对象</span>
			split = (FileSplit) context.getInputSplit();

			StringTokenizer itr = <span class="keyword">new</span> StringTokenizer(value.toString());

			<span class="keyword">while</span> (itr.hasMoreTokens()) {
				<span class="comment">// key 值由单词和URI组成，如“MapReduce:1.txt”</span>
				keyInfo.set(itr.nextToken() + <span class="string">":"</span> 
                        + split.getPath().toString());
			    valueInfo.set(<span class="string">"1"</span>);
			    context.write(keyInfo, valueInfo);
			}
		}
	}

	<span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">InvertedIndexCombiner</span> <span class="keyword">extends</span>
			<span class="title">Reducer</span>&<span class="title">lt</span>;<span class="title">Text</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">Text</span>&<span class="title">gt</span>; {</span>
		<span class="keyword">private</span> Text info = <span class="keyword">new</span> Text();

		<span class="keyword">public</span> <span class="keyword">void</span> reduce(Text key, Iterable&lt;Text&gt; values, Context context)
				<span class="keyword">throws</span> IOException, InterruptedException {
			<span class="comment">// 统计词频</span>
			<span class="keyword">int</span> sum = <span class="number">0</span>;
			<span class="keyword">for</span> (Text value : values) {
				sum += Integer.parseInt(value.toString());
			}

			<span class="keyword">int</span> splitIndex = key.toString().indexOf(<span class="string">":"</span>);
			<span class="comment">// 重新设置value值由URI和词频组成</span>
			info.set(key.toString().substring(splitIndex + <span class="number">1</span>) + <span class="string">":"</span> + sum);
			<span class="comment">// 重新设置key值为单词</span>
			key.set(key.toString().substring(<span class="number">0</span>, splitIndex));
			context.write(key, info);

		}
	}

	<span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">InvertedIndexReducer</span> <span class="keyword">extends</span>
			<span class="title">Reducer</span>&<span class="title">lt</span>;<span class="title">Text</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">Text</span>&<span class="title">gt</span>; {</span>
		<span class="keyword">private</span> Text result = <span class="keyword">new</span> Text();

		<span class="keyword">public</span> <span class="keyword">void</span> reduce(Text key, Iterable&lt;Text&gt; values, Context context)
				<span class="keyword">throws</span> IOException, InterruptedException {
			<span class="comment">// 生成文档列表</span>
			String fileList = <span class="keyword">new</span> String();
			<span class="keyword">for</span> (Text value : values) {
				fileList += value.toString() + <span class="string">";"</span>;
			}

			result.set(fileList);

			context.write(key, result);
		}
	}

	<span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> main(String[] args) <span class="keyword">throws</span> Exception {
		Configuration conf = <span class="keyword">new</span> Configuration();
		String[] otherArgs = <span class="keyword">new</span> GenericOptionsParser(conf, args)
				.getRemainingArgs();
		<span class="keyword">if</span> (otherArgs.length != <span class="number">2</span>) {
			System.out.println(<span class="string">"Usage: invertedIndex &lt;in&gt; &lt;out&gt;"</span>);
			System.exit(<span class="number">2</span>);
		}

		Job job = <span class="keyword">new</span> Job(conf, <span class="string">"InvertedIndex"</span>);
		job.setJarByClass(InvertedIndex.class);
		job.setMapperClass(InvertedIndexMapper.class);
		job.setMapOutputKeyClass(Text.class);
		job.setMapOutputValueClass(Text.class);
		job.setCombinerClass(InvertedIndexCombiner.class);
		job.setReducerClass(InvertedIndexReducer.class);
		job.setOutputKeyClass(Text.class);
		job.setOutputValueClass(Text.class);

		FileInputFormat.addInputPath(job, <span class="keyword">new</span> Path(otherArgs[<span class="number">0</span>]));
		FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(otherArgs[<span class="number">1</span>]));
		System.exit(job.waitForCompletion(<span class="keyword">true</span>) ? <span class="number">0</span> : <span class="number">1</span>);
	}

}
</pre></td></tr></table></figure>

<p><strong>参考：</strong><br>[1]. 《实战Hadoop》</p>

      
    </div>
    <footer>
      
        
          <div class="alignleft">
            <a href="/2013/04/18/invertedindex-in-hadoop/#more" class="more-link">Read More</a>
          </div>
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>



  
    <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2013-04-06T07:14:42.000Z"><a href="/2013/04/06/hdfs-java-api/">Apr 6 2013</a></time>
      
      
  
    <h1 class="title"><a href="/2013/04/06/hdfs-java-api/">理解 Hadoop 的 Java API</a></h1>
  

    </header>
    <div class="entry">
      
        <h3>案例</h3>
<h4>上传本地文件到 HDFS</h4>
<figure class="highlight lang-Java"><table><tr><td class="gutter"><pre>1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
</pre></td><td class="code"><pre><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> copyFile(String src, String dst, String config) <span class="keyword">throws</span> IOException{
    Configuration conf = <span class="keyword">new</span> Configuration();
    conf.addResource(<span class="keyword">new</span> Path(config));
    FileSystem hdfs = FileSystem.get(conf);
    Path srcPath = <span class="keyword">new</span> Path(src);
    Path dstPath = <span class="keyword">new</span> Path(dst);
    hdfs.copyFromLocalFile(srcPath, dstPath);
    System.out.println(<span class="string">"Upload to "</span> + conf.get(<span class="string">"fs.default.name"</span>));
    
    FileStatus files[] = hdfs.listStatus(dstPath);
    <span class="keyword">for</span> (FileStatus file : files) {
        System.out.println(file.getPath());
    }
    hdfs.close();
}
</pre></td></tr></table></figure>

<h4>创建 HDFS 文件</h4>
<figure class="highlight lang-Java"><table><tr><td class="gutter"><pre>1
2
3
4
5
6
7
8
9
10
11
12
13
</pre></td><td class="code"><pre><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> createFile(String dst, String config) <span class="keyword">throws</span> IOException{
    Configuration conf = <span class="keyword">new</span> Configuration();
    conf.addResource(<span class="keyword">new</span> Path(config));
    String content = <span class="string">"Hello World,beforeload"</span>;
    <span class="keyword">byte</span>[] buff = content.getBytes();
    FileSystem hdfs = FileSystem.get(conf);
    Path dfsPath = <span class="keyword">new</span> Path(dst);
    FSDataOutputStream os = hdfs.create(dfsPath);
    os.write(buff,<span class="number">0</span>,buff.length);
    os.write(content.getBytes(<span class="string">"UTF-8"</span>));
    os.close();
    hdfs.close();
}
</pre></td></tr></table></figure>

<h4>重命名 HDFS 文件</h4>
<figure class="highlight lang-Java"><table><tr><td class="gutter"><pre>1
2
3
4
5
6
7
8
9
</pre></td><td class="code"><pre><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> rename(String oldName, String newName, String config)
        <span class="keyword">throws</span> IOException {
    Configuration conf = <span class="keyword">new</span> Configuration();
    conf.addResource(<span class="keyword">new</span> Path(config));
    FileSystem hdfs = FileSystem.get(conf);
    Path oldPath = <span class="keyword">new</span> Path(oldName);
    Path newPath = <span class="keyword">new</span> Path(newName);
    <span class="keyword">boolean</span> isRename = hdfs.rename(oldPath, newPath);
}
</pre></td></tr></table></figure>

<h4>删除 HDFS 上的文件</h4>
<figure class="highlight lang-Java"><table><tr><td class="gutter"><pre>1
2
3
4
5
6
7
8
9
10
</pre></td><td class="code"><pre><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> deleteFile(String path, String config) <span class="keyword">throws</span> IOException {
    Configuration conf = <span class="keyword">new</span> Configuration();
    conf.addResource(<span class="keyword">new</span> Path(config));
    FileSystem hdfs = FileSystem.get(conf);
    Path deletePath = <span class="keyword">new</span> Path(path);
    <span class="keyword">boolean</span> isDeleted = hdfs.delete(deletePath, <span class="keyword">false</span>);
    <span class="comment">// 递归删除</span>
    <span class="comment">// boolean isDelete = hdfs.delete(deletePath, true);</span>
    System.out.println(<span class="string">"delete? "</span>+ isDeleted);
}
</pre></td></tr></table></figure>

<h4>查看 HDFS 文件的最后修改时间</h4>
<figure class="highlight lang-Java"><table><tr><td class="gutter"><pre>1
2
3
4
5
6
7
8
9
</pre></td><td class="code"><pre><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> getTime(String path, String config) <span class="keyword">throws</span> IOException{
    Configuration conf = <span class="keyword">new</span> Configuration();
    conf.addResource(<span class="keyword">new</span> Path(config));
    FileSystem hdfs = FileSystem.get(conf);
    Path filePath = <span class="keyword">new</span> Path(path);
    FileStatus fileStatus = hdfs.getFileStatus(filePath);
    <span class="keyword">long</span> modifyTime = fileStatus.getModificationTime();
    System.out.println(<span class="string">"Modification time is:"</span> + modifyTime);
}
</pre></td></tr></table></figure>

<h4>查看某个 HDFS 文件是否存在</h4>
<figure class="highlight lang-Java"><table><tr><td class="gutter"><pre>1
2
3
4
5
6
7
</pre></td><td class="code"><pre><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> isExist(String path, String config) <span class="keyword">throws</span> IOException {
    Configuration conf = <span class="keyword">new</span> Configuration();
    conf.addResource(<span class="keyword">new</span> Path(config));
    FileSystem hdfs = FileSystem.get(conf);
    <span class="keyword">boolean</span> isExist = hdfs.exists(<span class="keyword">new</span> Path(path));
    System.out.println(<span class="string">"Exist?"</span>+ isExist);
}
</pre></td></tr></table></figure>

<h4>查找某个文件在 HDFS 集群的位置</h4>
<figure class="highlight lang-Java"><table><tr><td class="gutter"><pre>1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
</pre></td><td class="code"><pre><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> getFileBlockLocation(String path, String config)
        <span class="keyword">throws</span> IOException {
    Configuration conf = <span class="keyword">new</span> Configuration();
    conf.addResource(<span class="keyword">new</span> Path(config));
    FileSystem hdfs = FileSystem.get(conf);
    Path filePath = <span class="keyword">new</span> Path(path);
    FileStatus fileStatus = hdfs.getFileStatus(filePath);
    BlockLocation[] blockLocations = hdfs.getFileBlockLocations(fileStatus,
            <span class="number">0</span>, fileStatus.getLen());

    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; blockLocations.length; i++) {
        String[] hosts = blockLocations[i].getHosts();
        System.out.println(<span class="string">"block"</span> + i + <span class="string">"location:"</span> + hosts[i]);
    }
}
</pre></td></tr></table></figure>

<h4>获取 HDFS 集群上所有节点的名称</h4>
<figure class="highlight lang-Java"><table><tr><td class="gutter"><pre>1
2
3
4
5
6
7
8
9
10
11
12
</pre></td><td class="code"><pre><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> getHostName(String config) <span class="keyword">throws</span> IOException {
    Configuration conf = <span class="keyword">new</span> Configuration();
    conf.addResource(<span class="keyword">new</span> Path(config));
    FileSystem fs = FileSystem.get(conf);
    DistributedFileSystem hdfs = (DistributedFileSystem) fs;
    DatanodeInfo[] dataNodeStats = hdfs.getDataNodeStats();
    String[] names = <span class="keyword">new</span> String[dataNodeStats.length];
    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; dataNodeStats.length; i++) {
        names[i] = dataNodeStats[i].getHostName();
        System.out.println(<span class="string">"node "</span> + i + <span class="string">" name "</span> + names[i]);
    }
}
</pre></td></tr></table></figure>

<h3>问题</h3>
<ol>
<li>&quot;Wrong FS expected: file:///&quot;</li>
</ol>
<p>这个问题其实严格意义上并不属于API调用方面的问题，具体问题出现的原因不得而知，不过在查阅资料一番后还是得出了问题的解决方法。</p>
<ul>
<li>stackoverflow上给出<a href="http://stackoverflow.com/questions/7969519/what-is-the-loading-order-of-the-configuration-files-in-hadoop/7995180#7995180">问题</a>的解决方法，不过经过尝试后，也只能发出感叹：”It doesn&#39;t work!&quot; </li>
<li>幸好在<a href="http://www.opensourceconnections.com/2013/03/24/hdfs-debugging-wrong-fs-expected-file-exception">Doug的博客</a>上给出了解答，通过添加一行代码即可<figure class="highlight lang-Java"><table><tr><td class="gutter"><pre>1
</pre></td><td class="code"><pre>conf.addResource(<span class="keyword">new</span> Path(<span class="string">"/root/hadoop-0.20.2/conf/core-site.xml"</span>));
</pre></td></tr></table></figure>

</li>
</ul>
<h3>讨论</h3>
<p><strong><em>附源码如下：</em></strong></p>
<figure class="highlight lang-Java"><table><tr><td class="gutter"><pre>1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
</pre></td><td class="code"><pre><span class="keyword">import</span> java.io.IOException;

<span class="keyword">import</span> org.apache.hadoop.conf.Configuration;
<span class="keyword">import</span> org.apache.hadoop.fs.BlockLocation;
<span class="keyword">import</span> org.apache.hadoop.fs.FSDataOutputStream;
<span class="keyword">import</span> org.apache.hadoop.fs.FileStatus;
<span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;
<span class="keyword">import</span> org.apache.hadoop.fs.Path;
<span class="keyword">import</span> org.apache.hadoop.hdfs.DistributedFileSystem;
<span class="keyword">import</span> org.apache.hadoop.hdfs.protocol.DatanodeInfo;

<span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Utils</span> {</span>
	<span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> main(String[] args) <span class="keyword">throws</span> IOException {
		String src, dst1, config, dst2, oldName, newName;
		src = <span class="string">"/root/word.txt"</span>;
		dst1 = <span class="string">"/"</span>;
		config = <span class="string">"/root/hadoop-0.20.2/conf/core-site.xml"</span>;
		<span class="comment">// copyFile(src, dst1, config);</span>

		dst2 = <span class="string">"/test.txt"</span>;
		<span class="comment">// createFile(dst2, config);</span>

		oldName = dst2;
		newName = <span class="string">"/test1.txt"</span>;
		<span class="comment">// rename(oldName, newName, config);</span>

		<span class="comment">// deleteFile(dst2, config);</span>
		<span class="comment">// getTime(dst1, config);</span>

		<span class="comment">// isExist(dst2, config);</span>
		<span class="comment">// isExist(dst1, config);</span>

		<span class="comment">// getFileBlockLocation(dst2, config);</span>

		getHostName(config);
	}

	<span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> copyFile(String src, String dst, String config)
			<span class="keyword">throws</span> IOException {
		Configuration conf = <span class="keyword">new</span> Configuration();
		conf.addResource(<span class="keyword">new</span> Path(config));
		FileSystem hdfs = FileSystem.get(conf);
		Path srcPath = <span class="keyword">new</span> Path(src);
		Path dstPath = <span class="keyword">new</span> Path(dst);
		hdfs.copyFromLocalFile(srcPath, dstPath);
		System.out.println(<span class="string">"Upload to "</span> + conf.get(<span class="string">"fs.default.name"</span>));

		FileStatus files[] = hdfs.listStatus(dstPath);
		<span class="keyword">for</span> (FileStatus file : files) {
			System.out.println(file.getPath());
		}
		hdfs.close();
	}

	<span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> createFile(String dst, String config) <span class="keyword">throws</span> IOException {
		Configuration conf = <span class="keyword">new</span> Configuration();
		conf.addResource(<span class="keyword">new</span> Path(config));
		String content = <span class="string">"Hello World,beforeload"</span>;
		<span class="keyword">byte</span>[] buff = content.getBytes();
		FileSystem hdfs = FileSystem.get(conf);
		Path dfsPath = <span class="keyword">new</span> Path(dst);
		FSDataOutputStream os = hdfs.create(dfsPath);
		os.write(buff, <span class="number">0</span>, buff.length);
		os.write(content.getBytes(<span class="string">"UTF-8"</span>));
		os.close();
		hdfs.close();
	}

	<span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> rename(String oldName, String newName, String config)
			<span class="keyword">throws</span> IOException {
		Configuration conf = <span class="keyword">new</span> Configuration();
		conf.addResource(<span class="keyword">new</span> Path(config));
		FileSystem hdfs = FileSystem.get(conf);
		Path oldPath = <span class="keyword">new</span> Path(oldName);
		Path newPath = <span class="keyword">new</span> Path(newName);
		<span class="keyword">boolean</span> isRename = hdfs.rename(oldPath, newPath);
	}

	<span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> deleteFile(String path, String config)
			<span class="keyword">throws</span> IOException {
		Configuration conf = <span class="keyword">new</span> Configuration();
		conf.addResource(<span class="keyword">new</span> Path(config));
		FileSystem hdfs = FileSystem.get(conf);
		Path deletePath = <span class="keyword">new</span> Path(path);
		<span class="keyword">boolean</span> isDeleted = hdfs.delete(deletePath, <span class="keyword">false</span>);
		<span class="comment">// 递归删除</span>
		<span class="comment">// boolean isDelete = hdfs.delete(deletePath, true);</span>
		System.out.println(<span class="string">"delete? "</span> + isDeleted);
	}

	<span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> getTime(String path, String config) <span class="keyword">throws</span> IOException {
		Configuration conf = <span class="keyword">new</span> Configuration();
		conf.addResource(<span class="keyword">new</span> Path(config));
		FileSystem hdfs = FileSystem.get(conf);
		Path filePath = <span class="keyword">new</span> Path(path);
		FileStatus fileStatus = hdfs.getFileStatus(filePath);
		<span class="keyword">long</span> modifyTime = fileStatus.getModificationTime();
		System.out.println(<span class="string">"Modification time is:"</span> + modifyTime);
	}

	<span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> isExist(String path, String config) <span class="keyword">throws</span> IOException {
		Configuration conf = <span class="keyword">new</span> Configuration();
		conf.addResource(<span class="keyword">new</span> Path(config));
		FileSystem hdfs = FileSystem.get(conf);
		<span class="keyword">boolean</span> isExist = hdfs.exists(<span class="keyword">new</span> Path(path));
		System.out.println(<span class="string">"Exist?"</span> + isExist);
	}

	<span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> getFileBlockLocation(String path, String config)
			<span class="keyword">throws</span> IOException {
		Configuration conf = <span class="keyword">new</span> Configuration();
		conf.addResource(<span class="keyword">new</span> Path(config));
		FileSystem hdfs = FileSystem.get(conf);
		Path filePath = <span class="keyword">new</span> Path(path);
		FileStatus fileStatus = hdfs.getFileStatus(filePath);
		BlockLocation[] blockLocations = hdfs.getFileBlockLocations(fileStatus,
				<span class="number">0</span>, fileStatus.getLen());

		<span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; blockLocations.length; i++) {
			String[] hosts = blockLocations[i].getHosts();
			System.out.println(<span class="string">"block"</span> + i + <span class="string">"location:"</span> + hosts[i]);
		}
	}

	<span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> getHostName(String config) <span class="keyword">throws</span> IOException {
		Configuration conf = <span class="keyword">new</span> Configuration();
		conf.addResource(<span class="keyword">new</span> Path(config));
		FileSystem fs = FileSystem.get(conf);
		DistributedFileSystem hdfs = (DistributedFileSystem) fs;
		DatanodeInfo[] dataNodeStats = hdfs.getDataNodeStats();
		String[] names = <span class="keyword">new</span> String[dataNodeStats.length];
		<span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; dataNodeStats.length; i++) {
			names[i] = dataNodeStats[i].getHostName();
			System.out.println(<span class="string">"node "</span> + i + <span class="string">" name "</span> + names[i]);
		}
	}
}
</pre></td></tr></table></figure>

<h4>参考书籍：</h4>
<ol>
<li>《Hadoop实战》—— &quot;Hadoop in Action&quot;</li>
<li>《实战Hadoop》—— &quot;开启通向云计算的捷径&quot;</li>
<li><a href="http://hadoop.apache.org/core/docs/current/api/org/apache/hadoop/fs/package-summary.html">Hadoop Java API 官方文档</a></li>
</ol>

      
    </div>
    <footer>
      
        
          <div class="alignleft">
            <a href="/2013/04/06/hdfs-java-api/#more" class="more-link">Read More</a>
          </div>
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>



  

  <nav id="pagination">
  
  
  <div class="clearfix"></div>
</nav>
</div></div>
    <aside id="sidebar" class="alignright">
  <div class="search">
  <form action="//google.com/search" method="get" accept-charset="utf-8">
    <input type="search" name="q" results="0" placeholder="搜索">
    <input type="hidden" name="q" value="site:beforeload.github.io">
  </form>
</div>

  
<div class="widget tag">
  <h3 class="title">分类</h3>
  <ul class="entry">
  
    <li><a href="/categories/Algorithm/">Algorithm</a><small>1</small></li>
  
    <li><a href="/categories/Data-Structure/">Data Structure</a><small>1</small></li>
  
    <li><a href="/categories/Hadoop/">Hadoop</a><small>3</small></li>
  
    <li><a href="/categories/Math/">Math</a><small>1</small></li>
  
    <li><a href="/categories/Memory/">Memory</a><small>2</small></li>
  
  </ul>
</div>


  
<div class="widget tag">
  <h3 class="title">标签</h3>
  <ul class="entry">
  
    <li><a href="/tags/API/">API</a><small>1</small></li>
  
    <li><a href="/tags/Algorithm/">Algorithm</a><small>2</small></li>
  
    <li><a href="/tags/C/">C</a><small>2</small></li>
  
    <li><a href="/tags/Data-Structure/">Data Structure</a><small>1</small></li>
  
    <li><a href="/tags/HDFS/">HDFS</a><small>1</small></li>
  
    <li><a href="/tags/Hadoop/">Hadoop</a><small>2</small></li>
  
    <li><a href="/tags/Java/">Java</a><small>2</small></li>
  
    <li><a href="/tags/MapReduce/">MapReduce</a><small>1</small></li>
  
    <li><a href="/tags/Math/">Math</a><small>1</small></li>
  
    <li><a href="/tags/Memory/">Memory</a><small>2</small></li>
  
  </ul>
</div>


  

</aside>
    <div class="clearfix"></div>
  </div>
  <footer id="footer" class="inner"><div class="alignleft">
  
  &copy; 2013 beforeload
  
</div>
<div class="clearfix"></div></footer>
  <script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>


<script type="text/javascript">
var disqus_shortname = 'beforeload';

(function(){
  var dsq = document.createElement('script');
  dsq.type = 'text/javascript';
  dsq.async = true;
  dsq.src = '//' + disqus_shortname + '.disqus.com/count.js';
  (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
}());
</script>



<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
(function($){
  $('.fancybox').fancybox();
})(jQuery);
</script>

</body>
</html>