<!DOCTYPE HTML>
<html>
<head>
  <meta charset="utf-8">
  <meta name="baidu-site-verification" content="OPRXXGXcV6onVshr" />
  
  <title>归档 | beforeload</title>
  <meta name="author" content="beforeload">
  
  <meta name="description" content="熟悉javascript，能写nodejs，看过Ruby，学过Java，对python也有兴趣，目前专注于C，搞搞Hadoop">
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  
  <meta property="og:site_name" content="beforeload"/>

  
    <meta property="og:image" content="undefined"/>
  

  <link href="/favicon.png" rel="icon">
  <link rel="alternate" href="http://beforeload.github.io/atom.xml" title="beforeload" type="application/atom+xml">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->
  <script src="//ajax.googleapis.com/ajax/libs/jquery/1.8/jquery.min.js"></script>
  
<script type="text/javascript">
  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-40059659-1']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();
</script>

</head>


<body>
  <header id="header" class="inner"><div class="alignleft">
  <h1><a href="/">beforeload</a></h1>
  <h2><a href="/">对编程世界充满激情的少年</a></h2>
</div>
<nav id="main-nav" class="alignright">
  <ul>
    
      <li><a href="/">Home</a></li>
    
      <li><a href="/archives">Archives</a></li>
    
      <li><a href="/atom.xml">RSS</a></li>
    
  </ul>
  <div class="clearfix"></div>
</nav>
<div class="clearfix"></div></header>
  <div id="content" class="inner">
    <div id="main-col" class="alignleft"><div id="wrapper">
<h2 class="archive-title">归档</h2>


  
    <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2013-04-22T13:23:27.000Z"><a href="/2013/04/22/collaborative-markov-chain-model/">Apr 22 2013</a></time>
      
      
  
    <h1 class="title"><a href="/2013/04/22/collaborative-markov-chain-model/">Collaborative Markov Chain Model</a></h1>
  

    </header>
    <div class="entry">
      
        <h2>理论</h2>
<h3>马尔可夫过程(Markov Process)</h3>
<p>Markov过程是一个满足Markov性(无后效性)的随机过程。
具有马尔可夫性质的随机过程称为<strong>马尔可夫</strong>过程。[1]</p>
<p>最有名的Markov过程是Markov链，但还有其他的过程，如布朗运动，也是Markov过程。</p>
<h3>马尔可夫性质</h3>
<p><strong>马尔可夫性质</strong>是概率论中的一个概念。当一个随即过程在给定状态及所有过去状态情况下，其未来状态的条件概率分布仅依赖于当前状态；换句话说，在给定现在状态时，它与过去状态（即该过程的历史路径）是条件独立的，那么此随机过程具有<strong>马尔可夫性质</strong>。[2]</p>
<p>过程或（系统）在时刻t0所处的状态为已知的条件下，过程在时刻t&gt;t0所处状态的条件分布，与过程在时刻t0之前所处的状态无关的特性称为马尔可夫性或无后效性。[1]即：已知“现在”过程的情况下，“<em>将来</em>”过程的情况与“<em>过去</em>”的情况无关。</p>
<p>数学上，如果&#39;X(t),t&gt;0&#39;为一个随机过程，则马尔可夫性质就是指</p>
<img src="//upload.wikimedia.org/math/7/a/d/7ad721c51e3fd880f548cbfa67e80832.png" class="[img]">


<p>马尔可夫过程通常称其为<strong>（时间）齐次</strong>，如果满足</p>
<img src="//upload.wikimedia.org/math/1/a/9/1a97955e98197a3ef3212d0cfba0a752.png" class="[img]">

<p>则称为<strong>（时间)非齐次</strong></p>
<h3>马尔可夫链</h3>
<p><strong>定义：</strong></p>
<p>马尔可夫链是随机变量<em>X1,X2,X3...</em>的一个数列。这些变量的范围，即他们所有可能取值的集合，被称为“状态空间”，而Xn的值则是在时间的状态。</p>
<p>已知随机变量Xn是n时刻的状态，如果Xn+1对于过去状态的条件概率分布仅是Xn的一个函数，则</p>
<p><code>P(Xn+1=x|X0,X1,X2,…,Xn)=P(Xn+1=x|Xn), x为过程中的某个状态。</code></p>
<p>时间和状态都是离散的Markov过程称之为Markov链，简记为马氏链。</p>
<h3>马氏链的转移概率(Transition probability)</h3>
<p>在经过一段较长时间的状态转移后，Markov过程会逐渐趋于稳定状态，且与初始状态无关，称为终极状态概率，或平衡状态概率。此时，记终极状态概率向量为X=[x1,x2,…,xn],则有X=Xp,0&lt;=Xi&lt;=1(I=1,2，…,n)，可用于预测Markov过程在未来出现什么趋势的重要信息。目前多应用于统计，生物，地理统计学，人力资源，因特网应用等多个领域。</p>
<h3>参考文献：</h3>
<ol>
<li><a href="http://wiki.mbalib.com/wiki/马尔可夫过程">马尔可夫过程 - MBA智库百科</a></li>
<li><a href="https://zh.wikipedia.org/wiki/马尔可夫性质">马尔可夫性质- 维基百科，自由的百科全书 - 维基百科- Wikipedia</a></li>
</ol>

      
    </div>
    <footer>
      
        
          <div class="alignleft">
            <a href="/2013/04/22/collaborative-markov-chain-model/#more" class="more-link">Read More</a>
          </div>
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>



  
    <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2013-04-18T04:26:32.000Z"><a href="/2013/04/18/invertedindex-in-hadoop/">Apr 18 2013</a></time>
      
      
  
    <h1 class="title"><a href="/2013/04/18/invertedindex-in-hadoop/">Inverted Index in Hadoop</a></h1>
  

    </header>
    <div class="entry">
      
        <h2>倒排索引</h2>
<h3>简介:</h3>
<p>倒排索引是文档检索系统中最常用的数据结构，被广泛地应用于全文搜索引擎。它主要是用来存储某个单词(或词组)在一个文档或一组文档中的存储位置的映射，即提供了一种根据内容来查找文档的方式。由于不是更具文档来确定文档包含的内容，而是进行想法的操作，因而成为倒排索引(Inverted Index)。[1]</p>
<p>我的理解就是找到单词出现的文档的名称，多数情况下为一个列表。</p>
<p>一个单词可能在不同的文件中出现，所以我们需要定义一个权重，表示单词(即搜索的内容)跟文档的<strong><em>相关度</em></strong>。相关度的衡量多数情况下用<strong><em>词频</em></strong>来表示。</p>
<p>更加复杂的算法TF-IDF(Term Frequency-Inverse Document Frequency)统计单词在多少个文档中出现，甚至考虑单词在文档中出现的位置(例如标题处反应这个单词的重要性)。</p>
<p>理论的东西到此结束，下面写一下倒排索引的设计与实现。</p>
<h3>问题分析：</h3>
<p>信息的关键： <strong>单词</strong>，<strong>文档URI</strong>及<strong>词频</strong></p>
<h3>设计：</h3>
<h4>Map过程：</h4>
<p>TextInputFormat: 输入文件处理 -&gt; 文本每行的偏移量及其内容<br><code>&lt;key, value&gt;</code> =&gt;  单词，文档URI和词频<br>两个值对应三个值，需要增加Combine过程进行词频统计。</p>
<p><strong><em>key</em></strong>： <strong>单词:URI</strong> (例如：MapReduce:1.txt)<br><strong><em>value</em></strong>：<strong>词频</strong>，相同单词词频组成列表传递给Combiner过程，实现的功能类似于WordCount      </p>
<h4>Combine过程：</h4>
<p>Combine过程会把相同的key值对应的value值累加<br>Map过程得到的结果为    </p>
<figure class="highlight lang-Java"><table><tr><td class="gutter"><pre>1
2
3
4
</pre></td><td class="code"><pre><span class="string">"MapRuduce:file01.txt"</span>  list(<span class="number">1</span>)          =&gt;     <span class="string">"MapReduce:file01.txt"</span>  <span class="number">1</span>    
<span class="string">"is:file01.txt"</span>         list(<span class="number">1</span>,<span class="number">1</span>)        =&gt;     <span class="string">"is:file01.txt"</span>         <span class="number">2</span>    
<span class="string">"powerful:file01.txt"</span>   list(<span class="number">1</span>)          =&gt;     <span class="string">"powerful:file01.txt"</span>   <span class="number">1</span>    
<span class="string">"simple:file01.txt"</span>     list(<span class="number">1</span>)          =&gt;     <span class="string">"simple:file01.txt"</span>     <span class="number">1</span>   
</pre></td></tr></table></figure>

<p><strong><em>key</em></strong>： <strong>单词</strong>
<strong><em>value</em></strong>: <strong>URI:词频</strong>(如：1.txt:1)</p>
<p><strong>好处</strong>：可以利用MapReduce框架默认的HashPartitioner类完成Shuffle过程。</p>
<h4>Reduce过程：</h4>
<p>Combiner过程就已经把相同的单词的所有记录发送给同一个Reducer进行处理，Reduce过程就变得很简单，只需要将相同的key和value值组合成倒排索引文件所需的格式即可，剩下的交给MapReducer框架自动完成。</p>
<h4>问题</h4>
<ol>
<li>文件数目;</li>
<li>文件大小;</li>
<li>Reduce过程没有统计词频，有可能会造成词频未统计完全的单词。    </li>
</ol>
<p><strong>备注及解决办法：</strong></p>
<ol>
<li>单个文件不宜过大，具体值与默认HDFS块大小及相关配置有关；</li>
<li>重写InputFormat类将每个文件作为一个split；</li>
<li>执行两次MapReduce，第一次统计词频，第二次MapReduce用于生成倒排索引。</li>
</ol>
<h4>优化思路：</h4>
<ol>
<li>利用复合键值对等实现包含更多信息的倒排索引。</li>
</ol>
<p><strong><strong>附Java源码：</strong></strong></p>
<figure class="highlight lang-Java"><table><tr><td class="gutter"><pre>1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
</pre></td><td class="code"><pre><span class="keyword">import</span> java.io.IOException;
<span class="keyword">import</span> java.util.StringTokenizer;

<span class="keyword">import</span> org.apache.hadoop.conf.Configuration;
<span class="keyword">import</span> org.apache.hadoop.fs.Path;
<span class="keyword">import</span> org.apache.hadoop.io.Text;
<span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;
<span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;
<span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;
<span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
<span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileSplit;
<span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
<span class="keyword">import</span> org.apache.hadoop.util.GenericOptionsParser;

<span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">InvertedIndex</span> {</span>
	<span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">InvertedIndexMapper</span> <span class="keyword">extends</span>
			<span class="title">Mapper</span>&<span class="title">lt</span>;<span class="title">Object</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">Text</span>&<span class="title">gt</span>; {</span>
		<span class="keyword">private</span> Text keyInfo = <span class="keyword">new</span> Text(); <span class="comment">// 存放单词和URI的组合</span>
		<span class="keyword">private</span> Text valueInfo = <span class="keyword">new</span> Text(); <span class="comment">// 存储词频</span>
		<span class="keyword">private</span> FileSplit split; <span class="comment">// 存储Split对象</span>

		<span class="keyword">public</span> <span class="keyword">void</span> map(Object key, Text value, Context context)
				<span class="keyword">throws</span> IOException, InterruptedException {

			<span class="comment">// 获得&lt;key, value&gt;对所属的FileSplit对象</span>
			split = (FileSplit) context.getInputSplit();

			StringTokenizer itr = <span class="keyword">new</span> StringTokenizer(value.toString());

			<span class="keyword">while</span> (itr.hasMoreTokens()) {
				<span class="comment">// key 值由单词和URI组成，如“MapReduce:1.txt”</span>
				keyInfo.set(itr.nextToken() + <span class="string">":"</span> 
                        + split.getPath().toString());
			    valueInfo.set(<span class="string">"1"</span>);
			    context.write(keyInfo, valueInfo);
			}
		}
	}

	<span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">InvertedIndexCombiner</span> <span class="keyword">extends</span>
			<span class="title">Reducer</span>&<span class="title">lt</span>;<span class="title">Text</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">Text</span>&<span class="title">gt</span>; {</span>
		<span class="keyword">private</span> Text info = <span class="keyword">new</span> Text();

		<span class="keyword">public</span> <span class="keyword">void</span> reduce(Text key, Iterable&lt;Text&gt; values, Context context)
				<span class="keyword">throws</span> IOException, InterruptedException {
			<span class="comment">// 统计词频</span>
			<span class="keyword">int</span> sum = <span class="number">0</span>;
			<span class="keyword">for</span> (Text value : values) {
				sum += Integer.parseInt(value.toString());
			}

			<span class="keyword">int</span> splitIndex = key.toString().indexOf(<span class="string">":"</span>);
			<span class="comment">// 重新设置value值由URI和词频组成</span>
			info.set(key.toString().substring(splitIndex + <span class="number">1</span>) + <span class="string">":"</span> + sum);
			<span class="comment">// 重新设置key值为单词</span>
			key.set(key.toString().substring(<span class="number">0</span>, splitIndex));
			context.write(key, info);

		}
	}

	<span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">InvertedIndexReducer</span> <span class="keyword">extends</span>
			<span class="title">Reducer</span>&<span class="title">lt</span>;<span class="title">Text</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">Text</span>&<span class="title">gt</span>; {</span>
		<span class="keyword">private</span> Text result = <span class="keyword">new</span> Text();

		<span class="keyword">public</span> <span class="keyword">void</span> reduce(Text key, Iterable&lt;Text&gt; values, Context context)
				<span class="keyword">throws</span> IOException, InterruptedException {
			<span class="comment">// 生成文档列表</span>
			String fileList = <span class="keyword">new</span> String();
			<span class="keyword">for</span> (Text value : values) {
				fileList += value.toString() + <span class="string">";"</span>;
			}

			result.set(fileList);

			context.write(key, result);
		}
	}

	<span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> main(String[] args) <span class="keyword">throws</span> Exception {
		Configuration conf = <span class="keyword">new</span> Configuration();
		String[] otherArgs = <span class="keyword">new</span> GenericOptionsParser(conf, args)
				.getRemainingArgs();
		<span class="keyword">if</span> (otherArgs.length != <span class="number">2</span>) {
			System.out.println(<span class="string">"Usage: invertedIndex &lt;in&gt; &lt;out&gt;"</span>);
			System.exit(<span class="number">2</span>);
		}

		Job job = <span class="keyword">new</span> Job(conf, <span class="string">"InvertedIndex"</span>);
		job.setJarByClass(InvertedIndex.class);
		job.setMapperClass(InvertedIndexMapper.class);
		job.setMapOutputKeyClass(Text.class);
		job.setMapOutputValueClass(Text.class);
		job.setCombinerClass(InvertedIndexCombiner.class);
		job.setReducerClass(InvertedIndexReducer.class);
		job.setOutputKeyClass(Text.class);
		job.setOutputValueClass(Text.class);

		FileInputFormat.addInputPath(job, <span class="keyword">new</span> Path(otherArgs[<span class="number">0</span>]));
		FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(otherArgs[<span class="number">1</span>]));
		System.exit(job.waitForCompletion(<span class="keyword">true</span>) ? <span class="number">0</span> : <span class="number">1</span>);
	}

}
</pre></td></tr></table></figure>

<p><strong>参考：</strong><br>[1]. 《实战Hadoop》</p>

      
    </div>
    <footer>
      
        
          <div class="alignleft">
            <a href="/2013/04/18/invertedindex-in-hadoop/#more" class="more-link">Read More</a>
          </div>
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>



  
    <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2013-04-12T18:56:30.000Z"><a href="/2013/04/13/analyze-mapreduce/">Apr 13 2013</a></time>
      
      
  
    <h1 class="title"><a href="/2013/04/13/analyze-mapreduce/">MapReduce</a></h1>
  

    </header>
    <div class="entry">
      
        <p>写在前面的话：看了N多MapReduce方面的理论知识，一直想写写自己对MapReduce的理解。</p>
<h2>MapReduce 编程模型</h2>
<figure class="highlight lang-Java"><table><tr><td class="gutter"><pre>1
2
</pre></td><td class="code"><pre>map:(K1, V1) -&gt; list(K2, V2)
reduce: (k2, list(V2)) -&gt; list(K2, V2)
</pre></td></tr></table></figure>

<p>简而言之就是 <strong><em> 输入-&gt; Mappers -&gt; 中间数据 -&gt; Reducer -&gt; 输出 </em></strong> 这样的一个过程，把输入<code>(key, value)</code>经过map和reduce函数转换成另一个或一批<code>(key, value)</code>对输出即可。</p>
<h3>Mapper</h3>
<p>Map阶段，MapReduce对任务输入数据分割，切割成固定大小的片段(splits)，对每个split进一步分解成一批键值对<code>(K1, V1)</code>。然后Hadoop为每个split创建Map任务(Mapper)，执行自定义的<code>map()</code>。</p>
<p>将split中的<code>(K1, V1)</code>键值对输入，得到结果为<code>(K2, V2)</code>的中间结果。<code>map()</code>的功能到这里并没有结束，因为我们在reduce阶段需要的输入格式是<code>(K2, list(V2))</code>，所以还需要对Mapper输出结果<code>(K2, V2)</code>进行合并(Combine过程)，即将中间结果中有相同key值(如：K2)的多组<code>(key, value)</code>对合并成一对(形成<code>(K2, list(V2))</code>)。key值范围决定了这些元组分组，对应不同的Reduce任务(Reducer)。</p>
<p><em>Tips:</em></p>
<ol>
<li>一个类作为mapper，要继承MapReduceBase基类并实现Mapper接口；</li>
<li>Mapper接口负责数据处理阶段。采用形式为<code>Mapper&lt;K1, V1, K2, V2&gt;</code> Java泛型；</li>
<li>Mapper只有一个方法——map，用于处理一个单独的键/值对。<figure class="highlight lang-Java"><table><tr><td class="gutter"><pre>1
2
</pre></td><td class="code"><pre><span class="keyword">void</span> map(K1 key, V1 value, OutputCollector&lt;K2, V2&gt; output, Reporter reporter)
    <span class="keyword">throws</span> IOException
</pre></td></tr></table></figure>

</li>
</ol>
<h3>Reducer</h3>
<p>Reduce阶段，数据整合，排序。然后调用自定义函数<code>reduce()</code>，输入<code>(K2, list(V2))</code>，得到键值对<code>&lt;K3, V3&gt;</code>输出到HDFS上。</p>
<p><em>Tips:</em></p>
<ol>
<li>Reducers数目在mapred-site.xml中决定，属性是<code>mapred.reduce.tasks</code>，默认值是 1，<code>job.setNumReduceTasks()</code>方法也可以用于设置，<strong>这是一个很重要的值</strong>；</li>
<li><p>reducer的实现首先必须在MapReduce基类上扩展，允许配置和清理。它必须实现Reducer接口实现<strong>reduce</strong>方法: </p>
<figure class="highlight lang-Java"><table><tr><td class="gutter"><pre>1
2
</pre></td><td class="code"><pre><span class="keyword">void</span> reduce(K2 key, Iterator&lt;V2&gt; values, OutputCollector&lt;K3, V3&gt; output, 
        Reporter reporter) <span class="keyword">throws</span> IOException
</pre></td></tr></table></figure>
</li>
<li><p><code>reduce()</code>函数最后生成的列表<code>(K3, V3)</code>可能为空；</p>
</li>
<li>map阶段和reduce阶段中间还有partitioner的工作：负责将mapper的结果输出给不同的reducer。</li>
</ol>
<h3>Hadoop的MapReduce</h3>
<p>Hadoop框架的核心是Map和Reduce操作，但不仅仅如此，还包括：</p>
<p><strong><em> data spliting(数据分割), shuffling(洗牌), Partitioning(分组), Combining(合并) </em></strong></p>
<p>以及各种格式的输入输出数据。</p>
<h3>Shuffler</h3>
<p>Mapper的按key值分为R份(R即为上面说到的Reducers的数目)，划分时通常采用hash函数，如<code>Hash(key) mod R</code>。目的是保证某一范围内的key一定由某个Reducer来处理。</p>
<p><em>Tips:</em></p>
<ol>
<li>洗牌之后相同的key对应的键值对放入相同的Reducer，不同的键也可以放入相同的Reducer。具体放入的位置由Partitioner决定。</li>
</ol>
<h3>Partitioner:重定向Mapper输出</h3>
<p>并不是数据排序好就是最好的。利用并行计算，不能仅仅靠一个reducer，那样就不是“云”而是“雨点”。当多个reducer一起使用时，默认的做法是对键值对进行hash来确定reducer。</p>
<p><em>Tips:</em></p>
<ol>
<li>Hadoop通过HashPartitioner类强制执行Partitioner策略。但HashPartitioner有时会出错；</li>
<li>量身定制partitioner，只需要实现<code>configure()</code>和<code>getPartition()</code>两个函数，前者将Hadoop对作业的配置应用在patitioner上，后者返回一个0到reduce任务数之间的整数，指向键/值对将要发送到的reducer。</li>
</ol>
<h3>Combiner: 本地reduce</h3>
<p>合并Mapper输出，即将多个key相同的<code>&lt;key, value&gt;</code>合并成一对。Combine过程和Reduce过程类似，很多情况下可以直接使用reduce函数，但Combiner过程是Mapper的一部分，在map函数后执行。</p>
<p><em>Tips：</em></p>
<ol>
<li>Hadoop并不保证对一个Mapper执行多少次Combine过程，所以我们应该做到无论Combine过程执行多少次，得到结果都一样；</li>
<li>中间结果的读取，JobTracker介入，负责通知中间文件的位置；</li>
<li>Mapper输出结果不在HDFS上而在本地磁盘上，出于时效性考虑，任务结束后删除，而HDFS的备份机制会造成性能损失，没有必要。</li>
</ol>
<h3>讨论：</h3>
<p>很多时候Rudecer产生的R个结果不是我们真正需要的最终结果，此时会把R个结果作为另一个计算的输入，开始另一个MapReduce任务，即任务管道。</p>
<h3>总结:</h3>
<p>MapReduce的集群行为(即MapReduce运行在大规模集群上的过程)，要完成一个并行计算，需要<em>_</em>任务调度与执行，本地计算，Shuffle，合并Mapper输出，读取中间结果，任务管道等一系列环节共同支撑计算的过程。</p>

      
    </div>
    <footer>
      
        
          <div class="alignleft">
            <a href="/2013/04/13/analyze-mapreduce/#more" class="more-link">Read More</a>
          </div>
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>



  
    <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2013-04-12T05:11:40.000Z"><a href="/2013/04/12/migrate_jekyll_to_hexo/">Apr 12 2013</a></time>
      
      
  
    <h1 class="title"><a href="/2013/04/12/migrate_jekyll_to_hexo/">From Jekyll to Hexo</a></h1>
  

    </header>
    <div class="entry">
      
        <h3>从Jekyll迁移到Hexo</h3>
<ol>
<li><a href="http://zespia.tw/hexo">Hexo</a>! </li>
<li><a href="http://zespia.tw/hexo/docs">documentation</a> 文档</li>
</ol>
<p><strong>优点：</strong></p>
<ul>
<li>真的很快，上手很easy</li>
</ul>
<p><strong>优待改进的地方：</strong></p>
<ul>
<li>主题欠缺，有时间改改主题</li>
</ul>

      
    </div>
    <footer>
      
        
          <div class="alignleft">
            <a href="/2013/04/12/migrate_jekyll_to_hexo/#more" class="more-link">Read More</a>
          </div>
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>



  
    <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2013-04-09T05:14:20.000Z"><a href="/2013/04/09/DFS-and-BFS/">Apr 9 2013</a></time>
      
      
  
    <h1 class="title"><a href="/2013/04/09/DFS-and-BFS/">理解深度优先和广度优先</a></h1>
  

    </header>
    <div class="entry">
      
        <h4>问题：</h4>
<p>迷宫, 1为墙壁， 0为可以走的路， 只能横着走和竖着走，求解左上角到右下角的路线。</p>
<figure class="highlight lang-Java"><table><tr><td class="gutter"><pre>1
2
3
4
5
6
</pre></td><td class="code"><pre><span class="keyword">int</span> maze[<span class="number">5</span>][<span class="number">5</span>] = {
    <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>,
    <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>,
    <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>,
    <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>,
};
</pre></td></tr></table></figure>

<h3>思路：</h3>
<p><strong>解法一：</strong></p>
<p>深度优先搜索(DFS, Depth First Search):每次搜索完各个方向相邻的点之后，取其中一个相邻的点走下去，一直走到无路可走了再退回来(回溯)，取另一个相邻的点再走下去。</p>
<figure class="highlight lang-Java"><table><tr><td class="gutter"><pre>1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
</pre></td><td class="code"><pre>将起点标记为已走过并压栈；
<span class="keyword">while</span> (栈非空) {
    从栈顶弹出一个点P；
    <span class="keyword">if</span> (p这个点是终点) {
        <span class="keyword">break</span>;
    }
    沿右、下、左、上四个方向探索相邻的点;
    <span class="keyword">if</span> (和p相邻的点有路可走，并且还没走过) {
        将相邻的点标记为已走过并压栈，它的前趋就是p点;
    }
}
<span class="keyword">if</span> (p点是终点) {
    打印p点的坐标；
    <span class="keyword">while</span> (p点有前趋) {
        p点 = p点的前趋;
        打印p点的坐标;
    }
} <span class="keyword">else</span> {
    没有路线可以到达终点;
}
</pre></td></tr></table></figure>

<p><strong><em>附源码如下：</em></strong></p>
<figure class="highlight lang-Java"><table><tr><td class="gutter"><pre>1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
</pre></td><td class="code"><pre>#include &lt;stdio.h&gt;
#define MAX_ROW <span class="number">5</span>
#define MAX_COL <span class="number">5</span>

struct point {
    <span class="keyword">int</span> row, col;
} stack[<span class="number">512</span>];

<span class="keyword">int</span> top = <span class="number">0</span>;

<span class="keyword">void</span> push(struct point p)
{
	stack[top++] = p;
}

struct point pop(<span class="keyword">void</span>)
{
	<span class="keyword">return</span> stack[--top];
}

<span class="keyword">int</span> is_empty(<span class="keyword">void</span>)
{
	<span class="keyword">return</span> top == <span class="number">0</span>;
}

<span class="keyword">int</span> maze[MAX_ROW][MAX_COL] = {
	<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>,
	<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>,
	<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>,
	<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>,
	<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>,
};

<span class="keyword">void</span> print_maze(<span class="keyword">void</span>)
{
	<span class="keyword">int</span> i, j;
	<span class="keyword">for</span> (i = <span class="number">0</span>; i &lt; MAX_ROW; i++) {
		<span class="keyword">for</span> (j = <span class="number">0</span>; j &lt; MAX_COL; j++) {
			printf(<span class="string">"%d "</span>, maze[i][j]);
		}
		putchar(<span class="string">'\n'</span>);
	}
	printf(<span class="string">"************\n"</span>);
}

struct point predecessor[MAX_ROW][MAX_COL] = {
	{ {-<span class="number">1</span>, -<span class="number">1</span>}, {-<span class="number">1</span>, -<span class="number">1</span>}, {-<span class="number">1</span>, -<span class="number">1</span>}, {-<span class="number">1</span>, -<span class="number">1</span>}, {-<span class="number">1</span>, -<span class="number">1</span>}},
	{ {-<span class="number">1</span>, -<span class="number">1</span>}, {-<span class="number">1</span>, -<span class="number">1</span>}, {-<span class="number">1</span>, -<span class="number">1</span>}, {-<span class="number">1</span>, -<span class="number">1</span>}, {-<span class="number">1</span>, -<span class="number">1</span>}},
	{ {-<span class="number">1</span>, -<span class="number">1</span>}, {-<span class="number">1</span>, -<span class="number">1</span>}, {-<span class="number">1</span>, -<span class="number">1</span>}, {-<span class="number">1</span>, -<span class="number">1</span>}, {-<span class="number">1</span>, -<span class="number">1</span>}},
	{ {-<span class="number">1</span>, -<span class="number">1</span>}, {-<span class="number">1</span>, -<span class="number">1</span>}, {-<span class="number">1</span>, -<span class="number">1</span>}, {-<span class="number">1</span>, -<span class="number">1</span>}, {-<span class="number">1</span>, -<span class="number">1</span>}},
	{ {-<span class="number">1</span>, -<span class="number">1</span>}, {-<span class="number">1</span>, -<span class="number">1</span>}, {-<span class="number">1</span>, -<span class="number">1</span>}, {-<span class="number">1</span>, -<span class="number">1</span>}, {-<span class="number">1</span>, -<span class="number">1</span>}},
};

<span class="keyword">void</span> visit(<span class="keyword">int</span> row, <span class="keyword">int</span> col, struct point pre)
{
	struct point visit_point = {
		row, col
	};
	maze[row][col] = <span class="number">2</span>;
	predecessor[row][col] = pre;
	push(visit_point);
}

<span class="keyword">int</span> main(<span class="keyword">void</span>)
{
	struct point p = {
		<span class="number">0</span>, <span class="number">0</span>
	};
	maze[p.row][p.col] = <span class="number">2</span>;
	push(p);

	<span class="keyword">while</span> (!is_empty()) {
		p = pop();
		<span class="keyword">if</span> (p.row == MAX_ROW - <span class="number">1</span> && p.col == MAX_COL - <span class="number">1</span>) {
			<span class="keyword">break</span>;
		}
		<span class="keyword">if</span> (p.col + <span class="number">1</span> &lt; MAX_COL && maze[p.row][p.col + <span class="number">1</span>] == <span class="number">0</span>) {
			visit(p.row, p.col + <span class="number">1</span>, p);
		}
		<span class="keyword">if</span> (p.row + <span class="number">1</span> &lt; MAX_ROW && maze[p.row + <span class="number">1</span>][p.col] == <span class="number">0</span>) {
			visit(p.row + <span class="number">1</span>, p.col, p);
		}
		<span class="keyword">if</span> (p.col - <span class="number">1</span> &gt;= <span class="number">0</span> && maze[p.row][p.col - <span class="number">1</span>] == <span class="number">0</span>) {
			visit(p.row, p.col - <span class="number">1</span>, p);
		}
		<span class="keyword">if</span> (p.row - <span class="number">1</span> &gt;= <span class="number">0</span> && maze[p.row - <span class="number">1</span>][p.col] == <span class="number">0</span>) {
			visit(p.row - <span class="number">1</span>, p.col, p);
		}
		print_maze();
	}
	<span class="keyword">if</span> (p.col == MAX_COL - <span class="number">1</span> && p.row == MAX_ROW - <span class="number">1</span>) {
		printf(<span class="string">"(%d, %d)\n"</span>, p.row, p.col);
		<span class="keyword">while</span> (predecessor[p.row][p.col].row != -<span class="number">1</span>) {
			p = predecessor[p.row][p.col];
			printf(<span class="string">"(%d, %d)\n"</span>, p.row, p.col);
		}
	} <span class="keyword">else</span> {
		printf(<span class="string">"No path!\n"</span>);
	}
	<span class="keyword">return</span> <span class="number">0</span>;
}
</pre></td></tr></table></figure>

<p><strong>DFS 优化：</strong></p>
<p>代码没有什么难懂的地方，不过有很多可以优化的地方，例如在predecessor这个数据结构上，浪费了太多的存储空间，可以做以下优化：</p>
<ol>
<li><p>重新定义predecessor存储方式
<figure class="highlight lang-Java"><table><tr><td class="gutter"><pre>1
</pre></td><td class="code"><pre>struct point predecessor[MAX_ROW][MAX_COL] = { <span class="number">0</span> };
</pre></td></tr></table></figure>
所有的值定义为0,前趋点为上点，则将他赋值为1，前趋点为下点，则赋值为-1，前趋点为左点，则赋值为2,前趋点为右点，则赋值为-2；通过定义四个不同的值区分前趋点，减少了存储空间，相应的函数也要对应修改即可。</p>
</li>
<li><p>用递归取代predecessor数据结构</p>
</li>
</ol>
<p><strong>解法二：</strong>
广度优先搜索(BFS, Breadth First Search):
BFS沿各个方向上同时展开搜索，每个可以走通的方向轮流往前走一步。</p>
<figure class="highlight lang-Java"><table><tr><td class="gutter"><pre>1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
</pre></td><td class="code"><pre>将起点标记为已经走过的队列
<span class="keyword">while</span> (队列非空) {
    出队一个点p;
    <span class="keyword">if</span> (p为终点) {
        <span class="keyword">break</span>;
    }
    否则沿右下左上四个方向探索相邻的点
    <span class="keyword">if</span> (和p相邻的点有路走，且没有走过) {
        将相邻的点标记为已经走过并入队列，他的前趋就是刚出队的p点;
    }
    <span class="keyword">if</span> (p点是终点) {
        打印p点的坐标;
        <span class="keyword">while</span> (p点有前趋) {
            p点 = p 点的前趋;
            打印 p 点的坐标;
        }
    } <span class="keyword">else</span> {
        没有到达终点的路线;
    }
}
</pre></td></tr></table></figure>

<p>BFS相比较于DFS，BFS可以找到从起点到终点的最短路径。</p>
<p><strong><em>附源码如下:</em></strong></p>
<figure class="highlight lang-Java"><table><tr><td class="gutter"><pre>1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
</pre></td><td class="code"><pre>#include &lt;stdio.h&gt;
#define MAX_ROW <span class="number">5</span>
#define MAX_COL <span class="number">5</span>

struct point {
	<span class="keyword">int</span> row, col, predecessor;
} queue[<span class="number">512</span>];

<span class="keyword">int</span> head = <span class="number">0</span>, tail = <span class="number">0</span>;

<span class="keyword">void</span> enqueue(struct point p)
{
	queue[tail++] = p;
}

struct point dequeue(<span class="keyword">void</span>)
{
	<span class="keyword">return</span> queue[head++];
}

<span class="keyword">int</span> is_empty(<span class="keyword">void</span>)
{
	<span class="keyword">return</span> head == tail;
}

<span class="keyword">int</span> maze[MAX_ROW][MAX_COL] = {
	<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>,
	<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>,
	<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>,
	<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>,
	<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>,
};

<span class="keyword">void</span> print_maze(<span class="keyword">void</span>)
{
	<span class="keyword">int</span> i, j;
	<span class="keyword">for</span> (i = <span class="number">0</span>; i &lt; MAX_ROW; i++) {
		<span class="keyword">for</span> (j = <span class="number">0</span>; j &lt; MAX_COL; j++) {
			printf(<span class="string">"%d "</span>, maze[i][j]);
		}
		putchar(<span class="string">'\n'</span>);
	}
	printf(<span class="string">"************\n"</span>);
}

<span class="keyword">void</span> visit(<span class="keyword">int</span> row, <span class="keyword">int</span> col)
{
	struct point visit_point = {
		row, col, head - <span class="number">1</span>
	};
	maze[row][col] = <span class="number">2</span>;
	enqueue(visit_point);
}

<span class="keyword">int</span> main(<span class="keyword">void</span>)
{
	struct point p = {
		<span class="number">0</span>, <span class="number">0</span>, -<span class="number">1</span>
	};
	maze[p.row][p.col] = <span class="number">2</span>;
	enqueue(p);

	<span class="keyword">while</span> (!is_empty()) {
		p = dequeue();
		<span class="keyword">if</span> (p.row == MAX_ROW - <span class="number">1</span> && p.col == MAX_COL - <span class="number">1</span>) {
			<span class="keyword">break</span>;
		}
		<span class="keyword">if</span> (p.row + <span class="number">1</span> &lt; MAX_ROW && maze[p.row + <span class="number">1</span>][p.col] == <span class="number">0</span>) {
			visit(p.row + <span class="number">1</span>, p.col);
		}
		<span class="keyword">if</span> (p.col + <span class="number">1</span> &lt; MAX_COL && maze[p.row][p.col + <span class="number">1</span>] == <span class="number">0</span>) {
			visit(p.row, p.col + <span class="number">1</span>);
		}
		<span class="keyword">if</span> (p.col - <span class="number">1</span> &gt;= <span class="number">0</span> && maze[p.row][p.col - <span class="number">1</span>] == <span class="number">0</span>) {
			visit(p.row, p.col - <span class="number">1</span>);
		}
		<span class="keyword">if</span> (p.row - <span class="number">1</span> &gt;= <span class="number">0</span> && maze[p.row - <span class="number">1</span>][p.col] == <span class="number">0</span>) {
			visit(p.row - <span class="number">1</span>, p.col);
		}
		print_maze();
	}
	<span class="keyword">if</span> (p.row == MAX_ROW - <span class="number">1</span> && p.col == MAX_COL - <span class="number">1</span>) {
		printf(<span class="string">"(%d, %d)\n"</span>, p.row, p.col);
		<span class="keyword">while</span> (p.predecessor != -<span class="number">1</span>) {
			p = queue[p.predecessor];
			printf(<span class="string">"(%d, %d)\n"</span>, p.row, p.col);
		}
	} <span class="keyword">else</span> {
		printf(<span class="string">"No path!\n"</span>);
	}
}
</pre></td></tr></table></figure>

<h4>参考：</h4>
<ol>
<li>《Linux C编程一站式学习》<a href="http://learn.akae.cn/media/ch12.html">栈与队列</a></li>
</ol>

      
    </div>
    <footer>
      
        
          <div class="alignleft">
            <a href="/2013/04/09/DFS-and-BFS/#more" class="more-link">Read More</a>
          </div>
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>



  
    <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2013-04-06T07:14:42.000Z"><a href="/2013/04/06/hdfs-java-api/">Apr 6 2013</a></time>
      
      
  
    <h1 class="title"><a href="/2013/04/06/hdfs-java-api/">理解 Hadoop 的 Java API</a></h1>
  

    </header>
    <div class="entry">
      
        <h3>案例</h3>
<h4>上传本地文件到 HDFS</h4>
<figure class="highlight lang-Java"><table><tr><td class="gutter"><pre>1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
</pre></td><td class="code"><pre><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> copyFile(String src, String dst, String config) <span class="keyword">throws</span> IOException{
    Configuration conf = <span class="keyword">new</span> Configuration();
    conf.addResource(<span class="keyword">new</span> Path(config));
    FileSystem hdfs = FileSystem.get(conf);
    Path srcPath = <span class="keyword">new</span> Path(src);
    Path dstPath = <span class="keyword">new</span> Path(dst);
    hdfs.copyFromLocalFile(srcPath, dstPath);
    System.out.println(<span class="string">"Upload to "</span> + conf.get(<span class="string">"fs.default.name"</span>));
    
    FileStatus files[] = hdfs.listStatus(dstPath);
    <span class="keyword">for</span> (FileStatus file : files) {
        System.out.println(file.getPath());
    }
    hdfs.close();
}
</pre></td></tr></table></figure>

<h4>创建 HDFS 文件</h4>
<figure class="highlight lang-Java"><table><tr><td class="gutter"><pre>1
2
3
4
5
6
7
8
9
10
11
12
13
</pre></td><td class="code"><pre><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> createFile(String dst, String config) <span class="keyword">throws</span> IOException{
    Configuration conf = <span class="keyword">new</span> Configuration();
    conf.addResource(<span class="keyword">new</span> Path(config));
    String content = <span class="string">"Hello World,beforeload"</span>;
    <span class="keyword">byte</span>[] buff = content.getBytes();
    FileSystem hdfs = FileSystem.get(conf);
    Path dfsPath = <span class="keyword">new</span> Path(dst);
    FSDataOutputStream os = hdfs.create(dfsPath);
    os.write(buff,<span class="number">0</span>,buff.length);
    os.write(content.getBytes(<span class="string">"UTF-8"</span>));
    os.close();
    hdfs.close();
}
</pre></td></tr></table></figure>

<h4>重命名 HDFS 文件</h4>
<figure class="highlight lang-Java"><table><tr><td class="gutter"><pre>1
2
3
4
5
6
7
8
9
</pre></td><td class="code"><pre><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> rename(String oldName, String newName, String config)
        <span class="keyword">throws</span> IOException {
    Configuration conf = <span class="keyword">new</span> Configuration();
    conf.addResource(<span class="keyword">new</span> Path(config));
    FileSystem hdfs = FileSystem.get(conf);
    Path oldPath = <span class="keyword">new</span> Path(oldName);
    Path newPath = <span class="keyword">new</span> Path(newName);
    <span class="keyword">boolean</span> isRename = hdfs.rename(oldPath, newPath);
}
</pre></td></tr></table></figure>

<h4>删除 HDFS 上的文件</h4>
<figure class="highlight lang-Java"><table><tr><td class="gutter"><pre>1
2
3
4
5
6
7
8
9
10
</pre></td><td class="code"><pre><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> deleteFile(String path, String config) <span class="keyword">throws</span> IOException {
    Configuration conf = <span class="keyword">new</span> Configuration();
    conf.addResource(<span class="keyword">new</span> Path(config));
    FileSystem hdfs = FileSystem.get(conf);
    Path deletePath = <span class="keyword">new</span> Path(path);
    <span class="keyword">boolean</span> isDeleted = hdfs.delete(deletePath, <span class="keyword">false</span>);
    <span class="comment">// 递归删除</span>
    <span class="comment">// boolean isDelete = hdfs.delete(deletePath, true);</span>
    System.out.println(<span class="string">"delete? "</span>+ isDeleted);
}
</pre></td></tr></table></figure>

<h4>查看 HDFS 文件的最后修改时间</h4>
<figure class="highlight lang-Java"><table><tr><td class="gutter"><pre>1
2
3
4
5
6
7
8
9
</pre></td><td class="code"><pre><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> getTime(String path, String config) <span class="keyword">throws</span> IOException{
    Configuration conf = <span class="keyword">new</span> Configuration();
    conf.addResource(<span class="keyword">new</span> Path(config));
    FileSystem hdfs = FileSystem.get(conf);
    Path filePath = <span class="keyword">new</span> Path(path);
    FileStatus fileStatus = hdfs.getFileStatus(filePath);
    <span class="keyword">long</span> modifyTime = fileStatus.getModificationTime();
    System.out.println(<span class="string">"Modification time is:"</span> + modifyTime);
}
</pre></td></tr></table></figure>

<h4>查看某个 HDFS 文件是否存在</h4>
<figure class="highlight lang-Java"><table><tr><td class="gutter"><pre>1
2
3
4
5
6
7
</pre></td><td class="code"><pre><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> isExist(String path, String config) <span class="keyword">throws</span> IOException {
    Configuration conf = <span class="keyword">new</span> Configuration();
    conf.addResource(<span class="keyword">new</span> Path(config));
    FileSystem hdfs = FileSystem.get(conf);
    <span class="keyword">boolean</span> isExist = hdfs.exists(<span class="keyword">new</span> Path(path));
    System.out.println(<span class="string">"Exist?"</span>+ isExist);
}
</pre></td></tr></table></figure>

<h4>查找某个文件在 HDFS 集群的位置</h4>
<figure class="highlight lang-Java"><table><tr><td class="gutter"><pre>1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
</pre></td><td class="code"><pre><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> getFileBlockLocation(String path, String config)
        <span class="keyword">throws</span> IOException {
    Configuration conf = <span class="keyword">new</span> Configuration();
    conf.addResource(<span class="keyword">new</span> Path(config));
    FileSystem hdfs = FileSystem.get(conf);
    Path filePath = <span class="keyword">new</span> Path(path);
    FileStatus fileStatus = hdfs.getFileStatus(filePath);
    BlockLocation[] blockLocations = hdfs.getFileBlockLocations(fileStatus,
            <span class="number">0</span>, fileStatus.getLen());

    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; blockLocations.length; i++) {
        String[] hosts = blockLocations[i].getHosts();
        System.out.println(<span class="string">"block"</span> + i + <span class="string">"location:"</span> + hosts[i]);
    }
}
</pre></td></tr></table></figure>

<h4>获取 HDFS 集群上所有节点的名称</h4>
<figure class="highlight lang-Java"><table><tr><td class="gutter"><pre>1
2
3
4
5
6
7
8
9
10
11
12
</pre></td><td class="code"><pre><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> getHostName(String config) <span class="keyword">throws</span> IOException {
    Configuration conf = <span class="keyword">new</span> Configuration();
    conf.addResource(<span class="keyword">new</span> Path(config));
    FileSystem fs = FileSystem.get(conf);
    DistributedFileSystem hdfs = (DistributedFileSystem) fs;
    DatanodeInfo[] dataNodeStats = hdfs.getDataNodeStats();
    String[] names = <span class="keyword">new</span> String[dataNodeStats.length];
    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; dataNodeStats.length; i++) {
        names[i] = dataNodeStats[i].getHostName();
        System.out.println(<span class="string">"node "</span> + i + <span class="string">" name "</span> + names[i]);
    }
}
</pre></td></tr></table></figure>

<h3>问题</h3>
<ol>
<li>&quot;Wrong FS expected: file:///&quot;</li>
</ol>
<p>这个问题其实严格意义上并不属于API调用方面的问题，具体问题出现的原因不得而知，不过在查阅资料一番后还是得出了问题的解决方法。</p>
<ul>
<li>stackoverflow上给出<a href="http://stackoverflow.com/questions/7969519/what-is-the-loading-order-of-the-configuration-files-in-hadoop/7995180#7995180">问题</a>的解决方法，不过经过尝试后，也只能发出感叹：”It doesn&#39;t work!&quot; </li>
<li>幸好在<a href="http://www.opensourceconnections.com/2013/03/24/hdfs-debugging-wrong-fs-expected-file-exception">Doug的博客</a>上给出了解答，通过添加一行代码即可<figure class="highlight lang-Java"><table><tr><td class="gutter"><pre>1
</pre></td><td class="code"><pre>conf.addResource(<span class="keyword">new</span> Path(<span class="string">"/root/hadoop-0.20.2/conf/core-site.xml"</span>));
</pre></td></tr></table></figure>

</li>
</ul>
<h3>讨论</h3>
<p><strong><em>附源码如下：</em></strong></p>
<figure class="highlight lang-Java"><table><tr><td class="gutter"><pre>1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
</pre></td><td class="code"><pre><span class="keyword">import</span> java.io.IOException;

<span class="keyword">import</span> org.apache.hadoop.conf.Configuration;
<span class="keyword">import</span> org.apache.hadoop.fs.BlockLocation;
<span class="keyword">import</span> org.apache.hadoop.fs.FSDataOutputStream;
<span class="keyword">import</span> org.apache.hadoop.fs.FileStatus;
<span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;
<span class="keyword">import</span> org.apache.hadoop.fs.Path;
<span class="keyword">import</span> org.apache.hadoop.hdfs.DistributedFileSystem;
<span class="keyword">import</span> org.apache.hadoop.hdfs.protocol.DatanodeInfo;

<span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Utils</span> {</span>
	<span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> main(String[] args) <span class="keyword">throws</span> IOException {
		String src, dst1, config, dst2, oldName, newName;
		src = <span class="string">"/root/word.txt"</span>;
		dst1 = <span class="string">"/"</span>;
		config = <span class="string">"/root/hadoop-0.20.2/conf/core-site.xml"</span>;
		<span class="comment">// copyFile(src, dst1, config);</span>

		dst2 = <span class="string">"/test.txt"</span>;
		<span class="comment">// createFile(dst2, config);</span>

		oldName = dst2;
		newName = <span class="string">"/test1.txt"</span>;
		<span class="comment">// rename(oldName, newName, config);</span>

		<span class="comment">// deleteFile(dst2, config);</span>
		<span class="comment">// getTime(dst1, config);</span>

		<span class="comment">// isExist(dst2, config);</span>
		<span class="comment">// isExist(dst1, config);</span>

		<span class="comment">// getFileBlockLocation(dst2, config);</span>

		getHostName(config);
	}

	<span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> copyFile(String src, String dst, String config)
			<span class="keyword">throws</span> IOException {
		Configuration conf = <span class="keyword">new</span> Configuration();
		conf.addResource(<span class="keyword">new</span> Path(config));
		FileSystem hdfs = FileSystem.get(conf);
		Path srcPath = <span class="keyword">new</span> Path(src);
		Path dstPath = <span class="keyword">new</span> Path(dst);
		hdfs.copyFromLocalFile(srcPath, dstPath);
		System.out.println(<span class="string">"Upload to "</span> + conf.get(<span class="string">"fs.default.name"</span>));

		FileStatus files[] = hdfs.listStatus(dstPath);
		<span class="keyword">for</span> (FileStatus file : files) {
			System.out.println(file.getPath());
		}
		hdfs.close();
	}

	<span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> createFile(String dst, String config) <span class="keyword">throws</span> IOException {
		Configuration conf = <span class="keyword">new</span> Configuration();
		conf.addResource(<span class="keyword">new</span> Path(config));
		String content = <span class="string">"Hello World,beforeload"</span>;
		<span class="keyword">byte</span>[] buff = content.getBytes();
		FileSystem hdfs = FileSystem.get(conf);
		Path dfsPath = <span class="keyword">new</span> Path(dst);
		FSDataOutputStream os = hdfs.create(dfsPath);
		os.write(buff, <span class="number">0</span>, buff.length);
		os.write(content.getBytes(<span class="string">"UTF-8"</span>));
		os.close();
		hdfs.close();
	}

	<span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> rename(String oldName, String newName, String config)
			<span class="keyword">throws</span> IOException {
		Configuration conf = <span class="keyword">new</span> Configuration();
		conf.addResource(<span class="keyword">new</span> Path(config));
		FileSystem hdfs = FileSystem.get(conf);
		Path oldPath = <span class="keyword">new</span> Path(oldName);
		Path newPath = <span class="keyword">new</span> Path(newName);
		<span class="keyword">boolean</span> isRename = hdfs.rename(oldPath, newPath);
	}

	<span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> deleteFile(String path, String config)
			<span class="keyword">throws</span> IOException {
		Configuration conf = <span class="keyword">new</span> Configuration();
		conf.addResource(<span class="keyword">new</span> Path(config));
		FileSystem hdfs = FileSystem.get(conf);
		Path deletePath = <span class="keyword">new</span> Path(path);
		<span class="keyword">boolean</span> isDeleted = hdfs.delete(deletePath, <span class="keyword">false</span>);
		<span class="comment">// 递归删除</span>
		<span class="comment">// boolean isDelete = hdfs.delete(deletePath, true);</span>
		System.out.println(<span class="string">"delete? "</span> + isDeleted);
	}

	<span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> getTime(String path, String config) <span class="keyword">throws</span> IOException {
		Configuration conf = <span class="keyword">new</span> Configuration();
		conf.addResource(<span class="keyword">new</span> Path(config));
		FileSystem hdfs = FileSystem.get(conf);
		Path filePath = <span class="keyword">new</span> Path(path);
		FileStatus fileStatus = hdfs.getFileStatus(filePath);
		<span class="keyword">long</span> modifyTime = fileStatus.getModificationTime();
		System.out.println(<span class="string">"Modification time is:"</span> + modifyTime);
	}

	<span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> isExist(String path, String config) <span class="keyword">throws</span> IOException {
		Configuration conf = <span class="keyword">new</span> Configuration();
		conf.addResource(<span class="keyword">new</span> Path(config));
		FileSystem hdfs = FileSystem.get(conf);
		<span class="keyword">boolean</span> isExist = hdfs.exists(<span class="keyword">new</span> Path(path));
		System.out.println(<span class="string">"Exist?"</span> + isExist);
	}

	<span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> getFileBlockLocation(String path, String config)
			<span class="keyword">throws</span> IOException {
		Configuration conf = <span class="keyword">new</span> Configuration();
		conf.addResource(<span class="keyword">new</span> Path(config));
		FileSystem hdfs = FileSystem.get(conf);
		Path filePath = <span class="keyword">new</span> Path(path);
		FileStatus fileStatus = hdfs.getFileStatus(filePath);
		BlockLocation[] blockLocations = hdfs.getFileBlockLocations(fileStatus,
				<span class="number">0</span>, fileStatus.getLen());

		<span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; blockLocations.length; i++) {
			String[] hosts = blockLocations[i].getHosts();
			System.out.println(<span class="string">"block"</span> + i + <span class="string">"location:"</span> + hosts[i]);
		}
	}

	<span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> getHostName(String config) <span class="keyword">throws</span> IOException {
		Configuration conf = <span class="keyword">new</span> Configuration();
		conf.addResource(<span class="keyword">new</span> Path(config));
		FileSystem fs = FileSystem.get(conf);
		DistributedFileSystem hdfs = (DistributedFileSystem) fs;
		DatanodeInfo[] dataNodeStats = hdfs.getDataNodeStats();
		String[] names = <span class="keyword">new</span> String[dataNodeStats.length];
		<span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; dataNodeStats.length; i++) {
			names[i] = dataNodeStats[i].getHostName();
			System.out.println(<span class="string">"node "</span> + i + <span class="string">" name "</span> + names[i]);
		}
	}
}
</pre></td></tr></table></figure>

<h4>参考书籍：</h4>
<ol>
<li>《Hadoop实战》—— &quot;Hadoop in Action&quot;</li>
<li>《实战Hadoop》—— &quot;开启通向云计算的捷径&quot;</li>
<li><a href="http://hadoop.apache.org/core/docs/current/api/org/apache/hadoop/fs/package-summary.html">Hadoop Java API 官方文档</a></li>
</ol>

      
    </div>
    <footer>
      
        
          <div class="alignleft">
            <a href="/2013/04/06/hdfs-java-api/#more" class="more-link">Read More</a>
          </div>
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>



  
    <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2013-04-05T12:12:32.000Z"><a href="/2013/04/05/k-th-order-statistic/">Apr 5 2013</a></time>
      
      
  
    <h1 class="title"><a href="/2013/04/05/k-th-order-statistic/">k-th Order Statistic</a></h1>
  

    </header>
    <div class="entry">
      
        <h3>问题描述:</h3>
<ol>
<li>实现一个算法，在一组随机排列的数中找出最小的一个。你能想到的最直观的算法一定是Θ(n)的，想想有没有比Θ(n)更快的算法？</li>
<li>在一组随机排列的数中找出第二小的，这个问题比上一个稍复杂，你能不能想出Θ(n)的算法？</li>
<li>进一步泛化，在一组随机排列的数中找出第k小的，这个元素称为<code>k-th Order Statistic</code>。能想到的最直观的算法肯定是先把这些数排序然后取第k个，时间复杂度和排序算法相同，可以是Θ(nlgn)。这个问题虽然比前两个问题复杂，但它也有平均情况下时间复杂度是Θ(n)的算法</li>
</ol>
<h6>问题来源: <a href="http://learn.akae.cn/media/ch11s05.html">线性查找</a></h6>
<hr>
<h3>求解及算法：</h3>
<p>求一组数中最小值，对于数组而言是一个很easy且常见的问题。
解决算法也很容易:</p>
<pre><code><span class="keyword">int</span> minimum(<span class="keyword">int</span> arr[])
{
    <span class="keyword">int</span> j,temp;
    temp = arr[<span class="number">0</span>];

    <span class="keyword">for</span> (j = <span class="number">1</span>; j &lt; N; j++) {
        <span class="keyword">if</span>(temp &gt; arr[j]){
            swap(&amp;temp,&amp;arr[j]);
        }
    }
    <span class="keyword">return</span> temp;
}
</code></pre>
<p>算法复杂度为Θ(n),且只需要遍历一次即可得出结果；</p>
<ol>
<li>在数组中求解第二小值也不是一件难事，只要想清楚求解的思路和步骤，问题便可迎刃而解。
当然算法可能性多种多样，我所采用的方法简单粗暴：</li>
</ol>
<p><figure class="highlight lang-Java"><table><tr><td class="gutter"><pre>1
2
3
4
5
6
7
8
9
10
11
12
13
</pre></td><td class="code"><pre><span class="javadoc">/** 第二小的数 */</span>
<span class="keyword">int</span> second_min(<span class="keyword">int</span> arr[])
{
    <span class="keyword">int</span> min, smin, i;
        min = smin = arr[<span class="number">0</span>];
    <span class="keyword">for</span> (i = <span class="number">0</span>; i &lt; N; i++) {
        <span class="keyword">if</span> (arr[i] &lt;= min) {
            smin = min;
            min = arr[i];
        }
    }
    <span class="keyword">return</span> smin;
}
</pre></td></tr></table></figure>
在每次找到更小的数字时，把当前的数字赋值给第二小的数字，算法复杂度同样是Θ(n),且只需遍历一次即可得出结果。</p>
<ol>
<li>对于求<code>k-th Order Statistic</code>元素，常规的解法是先排序，后根据排出的顺序，得到第k小的值。时间复杂度等同与排序的时间复杂度。当然，我们不需要完全排出顺序也能得到结果，学过<code>quick sort</code>之后，根据<code>pivot</code>元素的位置与k值比较，稍作修改快排算法即可得出解法。
求解算法如下：</li>
</ol>
<pre><code>
int partition(int start, int end)
{
    int pivot, mid, i;
    pivot = arr[start];
    mid = start;
    for (i = start + 1; i &lt;= end; i++) {
        if (arr[i] &lt; pivot) {
            mid++;
            swap(&amp;arr[i], &amp;arr[mid]);
        }
    }
    swap(&amp;arr[mid], &amp;arr[start]);
    return mid;
}

int order_statistic(int start, int end, int k)
{
    int i;
    if (start &lt; end) {
        i = partition(start, end);
        if (k == i) {
            return i;
        } else if (k &gt; i) {
            order_statistic(i + 1, end, k);
        } else {
            order_statistic(start, i - 1, k);
        }
    }
}
</code></pre>

<p>算法复杂度等同与快排是Θ(nlgn)，平均复杂度为Θ(n)。</p>
<p>附上完整C语言代码：</p>
<pre><code>#include &lt;stdio.h&gt;
#define N 10
int arr[N] = { 3, 4, 1, 3, 5, 6, 7, 9, 1, 2 };

int partition(int start, int end)
{
    int pivot, mid, i;
    pivot = arr[start];
    mid = start;
    for (i = start + 1; i &lt;= end; i++) {
        if (arr[i] &lt; pivot) {
            mid++;
            swap(&amp;arr[i], &amp;arr[mid]);
        }
    }
    swap(&amp;arr[mid], &amp;arr[start]);
    return mid;
}

int order_statistic(int start, int end, int k)
{
    int i;
    if (start &lt; end) {
        i = partition(start, end);
        if (k == i) {
            return i;
        } else if (k &gt; i) {
            order_statistic(i + 1, end, k);
        } else {
            order_statistic(start, i - 1, k);
        }
    }
}

int swap(int *a, int *b)
{
    int temp;
    temp = *b;
    *b = *a;
    *a = temp;
}

int main(int argc, const char *argv[])
{
    int i, k = 3, result;
    result = order_statistic(0, N - 1, k);
    for (i = 0; i &lt; N; i++) {
        printf("%d ", arr[i]);
    }
    printf("\n");
    printf("The %d-th Order Statistic is %d.\n", k + 1, arr[result]);
    return 0;
}
</code></pre>

<h3>讨论</h3>
<hr>

      
    </div>
    <footer>
      
        
          <div class="alignleft">
            <a href="/2013/04/05/k-th-order-statistic/#more" class="more-link">Read More</a>
          </div>
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>



  
    <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2013-04-03T15:11:12.000Z"><a href="/2013/04/03/blog-establish/">Apr 3 2013</a></time>
      
      
  
    <h1 class="title"><a href="/2013/04/03/blog-establish/">blog establish</a></h1>
  

    </header>
    <div class="entry">
      
        <p>新建第一篇博文，以纪念今天博客建立</p>
<p>一直在犹豫使用Jekyll还是Octopress搭建自己的博客,虽然后者基于前者做了一些优化之类的东西,但最终犹豫再三还是使用了Jekyll。没有使用过Octopress，不知道两者之间的区别在什么地方，不过相对于初学者，使用Jekyll还是非常容易上手的，具体的步骤参看<a href="http://jekyllbootstrap.com">Jekyll Bootstrap</a>。</p>

      
    </div>
    <footer>
      
        
          <div class="alignleft">
            <a href="/2013/04/03/blog-establish/#more" class="more-link">Read More</a>
          </div>
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>



  

  <nav id="pagination">
  
  
  <div class="clearfix"></div>
</nav>
</div></div>
    <aside id="sidebar" class="alignright">
  <div class="search">
  <form action="//google.com/search" method="get" accept-charset="utf-8">
    <input type="search" name="q" results="0" placeholder="搜索">
    <input type="hidden" name="q" value="site:beforeload.github.io">
  </form>
</div>

  
<div class="widget tag">
  <h3 class="title">分类</h3>
  <ul class="entry">
  
    <li><a href="/categories/Algorithm/">Algorithm</a><small>1</small></li>
  
    <li><a href="/categories/Data-Structure/">Data Structure</a><small>1</small></li>
  
    <li><a href="/categories/Hadoop/">Hadoop</a><small>3</small></li>
  
    <li><a href="/categories/Math/">Math</a><small>1</small></li>
  
    <li><a href="/categories/Memory/">Memory</a><small>2</small></li>
  
  </ul>
</div>


  
<div class="widget tag">
  <h3 class="title">标签</h3>
  <ul class="entry">
  
    <li><a href="/tags/API/">API</a><small>1</small></li>
  
    <li><a href="/tags/Algorithm/">Algorithm</a><small>2</small></li>
  
    <li><a href="/tags/C/">C</a><small>2</small></li>
  
    <li><a href="/tags/Data-Structure/">Data Structure</a><small>1</small></li>
  
    <li><a href="/tags/HDFS/">HDFS</a><small>1</small></li>
  
    <li><a href="/tags/Hadoop/">Hadoop</a><small>2</small></li>
  
    <li><a href="/tags/Java/">Java</a><small>2</small></li>
  
    <li><a href="/tags/MapReduce/">MapReduce</a><small>1</small></li>
  
    <li><a href="/tags/Math/">Math</a><small>1</small></li>
  
    <li><a href="/tags/Memory/">Memory</a><small>2</small></li>
  
  </ul>
</div>


  

</aside>
    <div class="clearfix"></div>
  </div>
  <footer id="footer" class="inner"><div class="alignleft">
  
  &copy; 2013 beforeload
  
</div>
<div class="clearfix"></div></footer>
  <script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>


<script type="text/javascript">
var disqus_shortname = 'beforeload';

(function(){
  var dsq = document.createElement('script');
  dsq.type = 'text/javascript';
  dsq.async = true;
  dsq.src = '//' + disqus_shortname + '.disqus.com/count.js';
  (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
}());
</script>



<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
(function($){
  $('.fancybox').fancybox();
})(jQuery);
</script>

</body>
</html>